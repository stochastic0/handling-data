#Alpha vantage에서 프리미엄멤버에게만분당 5개 이상의 데이터를 제공하기에 프리미엄멤버가 아니신분들은 주석처리한 Sleep(61)을 적용하셔야합니다.
#프리미엄멤버가 아니신분들은 시간이 다소 걸립니다. 
#Since Alpha Vantage provides more than 5 data per minute only to premium members, non-premium members must apply  #Sleep(61)

import requests
from bs4 import BeautifulSoup as bs
import pandas as pd
from time import sleep

def get_sp500_tickers():
    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    soup = bs(resp.text, 'lxml')
    table = soup.find('table', {'class': 'wikitable sortable'})
    tickers = []
    for row in table.findAll('tr')[1:]:
        ticker = row.findAll('td')[0].text
        tickers.append(ticker)

    return tickers

sp500tickers = get_sp500_tickers()

for i in range(0,504):
    sp500tickers[i] = sp500tickers[i][:-1]

del sp500tickers[0]

#--------------------------------------------------1년치-----------------------------------------------------------------

symbol = 'MMM'
interval = '1min'
slice = 'year1month1'
api_key = ''  #alpha vantage api key 입력
adjusted = '&adjusted=true&'
csv_url = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol='+symbol+'&interval='+interval+'&slice='+slice+adjusted+'&apikey='+api_key
data = pd.read_csv(csv_url)
data.rename(columns = {'close':'MMM close'}, inplace = True)
data = data.iloc[:,[0,4]]

for i in range(2,12):
    symbol = 'MMM'
    interval = '1min'
    slice = 'year1month'+str(i)
    adjusted = '&adjusted=true&'
    csv_url = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=' + symbol + '&interval=' + interval + '&slice=' + slice + adjusted + '&apikey=' + api_key
    data_ = pd.read_csv(csv_url)
    data_.rename(columns={'close': 'MMM close'}, inplace=True)
    data_ = data_.iloc[:, [0, 4]]
    data = pd.concat([data,data_])
    #sleep(61)
    

slice = 'year1month12'
csv_url = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol='+symbol+'&interval='+interval+'&slice='+slice+adjusted+'&apikey='+api_key
data_12month = pd.read_csv(csv_url)
data_12month.rename(columns={'close': 'MMM close'}, inplace=True)
data_12month = data_12month.iloc[:, [0, 4]]

data = pd.concat([data, data_12month])

for ticker in sp500tickers:

    symbol = ticker
    interval = '1min'
    slice = 'year1month1'
    adjusted = '&adjusted=true&'
    csv_url = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=' + symbol + '&interval=' + interval + '&slice=' + slice + adjusted + '&apikey=' + api_key
    data1 = pd.read_csv(csv_url)
    data1.rename(columns={'close': ticker +' '+'close'}, inplace=True)
    data1 = data1.iloc[:, [0, 4]]

    for i in range(2,12):
        symbol = ticker
        interval = '1min'
        slice = 'year1month' + str(i)
        adjusted = '&adjusted=true&'
        csv_url = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=' + symbol + '&interval=' + interval + '&slice=' + slice + adjusted + '&apikey=' + api_key
        data1_ = pd.read_csv(csv_url)
        data1_.rename(columns={'close': ticker +' '+'close'}, inplace=True)
        data1_ = data1_.iloc[:, [0, 4]]
        data1 = pd.concat([data1, data1_])
        #sleep(61)

    slice = 'year1month12'
    csv_url = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=' + symbol + '&interval=' + interval + '&slice=' + slice + adjusted + '&apikey=' + api_key
    data_12month1 = pd.read_csv(csv_url)
    data_12month1.rename(columns={'close': ticker +' '+'close'}, inplace=True)
    data_12month1 = data_12month1.iloc[:, [0, 4]]

    data1 = pd.concat([data1, data_12month1])
    data = pd.merge(data,data1, how='outer')
